{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278636d5",
   "metadata": {},
   "source": [
    "# üß™ Docker Compose Jupyter Lab Integration Test\n",
    "\n",
    "This notebook validates the complete Docker Compose setup for Jupyter Lab, ensuring all components work correctly end-to-end.\n",
    "\n",
    "## Test Objectives\n",
    "- ‚úÖ Validate Docker Compose configuration\n",
    "- ‚úÖ Test Jupyter server connectivity and API\n",
    "- ‚úÖ Verify data science packages functionality\n",
    "- ‚úÖ Test volume mounts and file operations\n",
    "- ‚úÖ Generate comprehensive test reports\n",
    "- ‚úÖ Save results to scripts/tests/results folder\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca499ae",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for testing Docker Compose configuration and Jupyter functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239bbc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn import datasets\n",
    "\n",
    "# Test configuration\n",
    "TEST_CONFIG = {\n",
    "    \"base_url\": \"http://localhost:8888\",\n",
    "    \"token\": \"datascience-token\",\n",
    "    \"test_timestamp\": datetime.now().isoformat(),\n",
    "    \"project_root\": \"/home/jovyan/work\",\n",
    "    \"results_dir\": \"/home/jovyan/work/scripts/tests/results\"\n",
    "}\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"Test Configuration: {json.dumps(TEST_CONFIG, indent=2)}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545076ee",
   "metadata": {},
   "source": [
    "## 2. Setup Test Environment\n",
    "\n",
    "Create the directory structure and initialize test tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize test tracking\n",
    "test_results = []\n",
    "test_counter = 0\n",
    "\n",
    "def log_test_result(test_name, status, message=\"\", details=None):\n",
    "    \"\"\"Log test results with standardized format\"\"\"\n",
    "    global test_counter\n",
    "    test_counter += 1\n",
    "    \n",
    "    result = {\n",
    "        \"test_id\": test_counter,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"test_name\": test_name,\n",
    "        \"status\": status,\n",
    "        \"message\": message,\n",
    "        \"details\": details or {}\n",
    "    }\n",
    "    test_results.append(result)\n",
    "    \n",
    "    status_icon = \"‚úÖ\" if status == \"PASS\" else \"‚ùå\" if status == \"FAIL\" else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status_icon} Test {test_counter}: {test_name}\")\n",
    "    if message:\n",
    "        print(f\"   ‚îî‚îÄ‚îÄ {message}\")\n",
    "    return result\n",
    "\n",
    "# Create test directories\n",
    "tests_dir = Path(TEST_CONFIG[\"project_root\"]) / \"scripts\" / \"tests\"\n",
    "results_dir = Path(TEST_CONFIG[\"results_dir\"])\n",
    "\n",
    "tests_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_test_result(\"Test Environment Setup\", \"PASS\", f\"Created directories: {tests_dir}, {results_dir}\")\n",
    "\n",
    "print(f\"\\nüìÅ Test Environment Ready:\")\n",
    "print(f\"   Tests directory: {tests_dir}\")\n",
    "print(f\"   Results directory: {results_dir}\")\n",
    "print(f\"   Current directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c9ccb",
   "metadata": {},
   "source": [
    "## 3. Test Docker Compose Configuration\n",
    "\n",
    "Validate the Docker Compose setup and container environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24decc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Check if we're running in the expected container environment\n",
    "def test_container_environment():\n",
    "    \"\"\"Test if we're running in the correct Docker container\"\"\"\n",
    "    try:\n",
    "        # Check hostname (should be container ID)\n",
    "        hostname = os.uname().nodename\n",
    "        \n",
    "        # Check if we're running as jovyan user\n",
    "        user = os.environ.get('USER', 'unknown')\n",
    "        \n",
    "        # Check if conda is available (part of datascience-notebook)\n",
    "        conda_result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        \n",
    "        details = {\n",
    "            \"hostname\": hostname,\n",
    "            \"user\": user,\n",
    "            \"conda_available\": conda_result.returncode == 0,\n",
    "            \"conda_version\": conda_result.stdout.strip() if conda_result.returncode == 0 else \"Not available\"\n",
    "        }\n",
    "        \n",
    "        if user == 'jovyan' and conda_result.returncode == 0:\n",
    "            return log_test_result(\"Container Environment\", \"PASS\", f\"Running as {user} with conda\", details)\n",
    "        else:\n",
    "            return log_test_result(\"Container Environment\", \"FAIL\", f\"Unexpected environment: user={user}\", details)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return log_test_result(\"Container Environment\", \"FAIL\", f\"Error checking environment: {str(e)}\")\n",
    "\n",
    "# Test 2: Verify volume mounts\n",
    "def test_volume_mounts():\n",
    "    \"\"\"Test if all expected volume mounts are accessible\"\"\"\n",
    "    expected_paths = [\n",
    "        \"/home/jovyan/work/notebooks\",\n",
    "        \"/home/jovyan/work/data\", \n",
    "        \"/home/jovyan/work/scripts\",\n",
    "        \"/home/jovyan/work/outputs\"\n",
    "    ]\n",
    "    \n",
    "    accessible_paths = []\n",
    "    missing_paths = []\n",
    "    \n",
    "    for path in expected_paths:\n",
    "        if Path(path).exists():\n",
    "            accessible_paths.append(path)\n",
    "        else:\n",
    "            missing_paths.append(path)\n",
    "    \n",
    "    if not missing_paths:\n",
    "        return log_test_result(\"Volume Mounts\", \"PASS\", f\"All {len(expected_paths)} paths accessible\", \n",
    "                             {\"accessible_paths\": accessible_paths})\n",
    "    else:\n",
    "        return log_test_result(\"Volume Mounts\", \"FAIL\", f\"Missing paths: {missing_paths}\",\n",
    "                             {\"accessible_paths\": accessible_paths, \"missing_paths\": missing_paths})\n",
    "\n",
    "# Test 3: Check data science packages\n",
    "def test_data_science_packages():\n",
    "    \"\"\"Test availability and versions of key data science packages\"\"\"\n",
    "    packages_to_test = {\n",
    "        'pandas': pd,\n",
    "        'numpy': np, \n",
    "        'matplotlib': plt.matplotlib,\n",
    "        'seaborn': sns,\n",
    "        'plotly': px.plotly,\n",
    "        'sklearn': datasets.sklearn\n",
    "    }\n",
    "    \n",
    "    package_info = {}\n",
    "    failed_packages = []\n",
    "    \n",
    "    for name, module in packages_to_test.items():\n",
    "        try:\n",
    "            version = getattr(module, '__version__', 'Version not available')\n",
    "            package_info[name] = version\n",
    "        except Exception as e:\n",
    "            failed_packages.append(f\"{name}: {str(e)}\")\n",
    "    \n",
    "    if not failed_packages:\n",
    "        return log_test_result(\"Data Science Packages\", \"PASS\", \n",
    "                             f\"All {len(packages_to_test)} packages available\", package_info)\n",
    "    else:\n",
    "        return log_test_result(\"Data Science Packages\", \"FAIL\", \n",
    "                             f\"Failed packages: {failed_packages}\", package_info)\n",
    "\n",
    "# Run Docker Compose tests\n",
    "print(\"üê≥ Running Docker Compose Configuration Tests...\\n\")\n",
    "\n",
    "test_container_environment()\n",
    "test_volume_mounts() \n",
    "test_data_science_packages()\n",
    "\n",
    "print(f\"\\nüìä Docker Tests Completed: {len([r for r in test_results if r['test_name'] in ['Container Environment', 'Volume Mounts', 'Data Science Packages']])} tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415224d5",
   "metadata": {},
   "source": [
    "## 4. Test Jupyter Notebook Connectivity\n",
    "\n",
    "Verify Jupyter server functionality and API endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Jupyter Server Status\n",
    "def test_jupyter_server():\n",
    "    \"\"\"Test if Jupyter server is running and accessible internally\"\"\"\n",
    "    try:\n",
    "        # Check if we can access Jupyter internals\n",
    "        from IPython import get_ipython\n",
    "        from jupyter_client import find_connection_file\n",
    "        \n",
    "        ipython = get_ipython()\n",
    "        kernel_id = ipython.kernel.ident if ipython and hasattr(ipython, 'kernel') else 'unknown'\n",
    "        \n",
    "        # Try to find connection file\n",
    "        try:\n",
    "            connection_file = find_connection_file()\n",
    "            connection_available = True\n",
    "        except:\n",
    "            connection_file = \"Not found\"\n",
    "            connection_available = False\n",
    "        \n",
    "        details = {\n",
    "            \"ipython_available\": ipython is not None,\n",
    "            \"kernel_id\": str(kernel_id),\n",
    "            \"connection_file\": str(connection_file),\n",
    "            \"connection_available\": connection_available\n",
    "        }\n",
    "        \n",
    "        if ipython is not None:\n",
    "            return log_test_result(\"Jupyter Server Status\", \"PASS\", \n",
    "                                 \"IPython kernel running\", details)\n",
    "        else:\n",
    "            return log_test_result(\"Jupyter Server Status\", \"FAIL\", \n",
    "                                 \"IPython not available\", details)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return log_test_result(\"Jupyter Server Status\", \"FAIL\", \n",
    "                             f\"Error checking Jupyter: {str(e)}\")\n",
    "\n",
    "# Test 5: Kernel Functionality  \n",
    "def test_kernel_functionality():\n",
    "    \"\"\"Test kernel execution capabilities\"\"\"\n",
    "    try:\n",
    "        # Test basic Python execution\n",
    "        result = eval(\"2 + 2\")\n",
    "        \n",
    "        # Test variable assignment and retrieval\n",
    "        test_var = \"jupyter_test_variable\"\n",
    "        globals()[test_var] = \"test_value\"\n",
    "        var_test = globals().get(test_var) == \"test_value\"\n",
    "        \n",
    "        # Test import capabilities\n",
    "        import random\n",
    "        random_test = random.randint(1, 100)\n",
    "        \n",
    "        details = {\n",
    "            \"basic_math\": result == 4,\n",
    "            \"variable_assignment\": var_test,\n",
    "            \"import_test\": isinstance(random_test, int),\n",
    "            \"random_value\": random_test\n",
    "        }\n",
    "        \n",
    "        all_tests_passed = all(details.values())\n",
    "        \n",
    "        if all_tests_passed:\n",
    "            return log_test_result(\"Kernel Functionality\", \"PASS\", \n",
    "                                 \"All kernel operations working\", details)\n",
    "        else:\n",
    "            return log_test_result(\"Kernel Functionality\", \"FAIL\", \n",
    "                                 \"Some kernel operations failed\", details)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return log_test_result(\"Kernel Functionality\", \"FAIL\", \n",
    "                             f\"Kernel test error: {str(e)}\")\n",
    "\n",
    "# Test 6: File System Operations\n",
    "def test_filesystem_operations():\n",
    "    \"\"\"Test file system read/write operations\"\"\"\n",
    "    try:\n",
    "        test_file_path = Path(\"/home/jovyan/work/notebooks/kernel_test.txt\")\n",
    "        test_content = f\"Kernel test file created at {datetime.now().isoformat()}\"\n",
    "        \n",
    "        # Write test\n",
    "        with open(test_file_path, 'w') as f:\n",
    "            f.write(test_content)\n",
    "        write_success = test_file_path.exists()\n",
    "        \n",
    "        # Read test\n",
    "        if write_success:\n",
    "            with open(test_file_path, 'r') as f:\n",
    "                read_content = f.read()\n",
    "            read_success = read_content == test_content\n",
    "            \n",
    "            # Cleanup\n",
    "            test_file_path.unlink()\n",
    "            cleanup_success = not test_file_path.exists()\n",
    "        else:\n",
    "            read_success = False\n",
    "            cleanup_success = False\n",
    "        \n",
    "        details = {\n",
    "            \"write_operation\": write_success,\n",
    "            \"read_operation\": read_success, \n",
    "            \"cleanup_operation\": cleanup_success,\n",
    "            \"file_path\": str(test_file_path)\n",
    "        }\n",
    "        \n",
    "        if write_success and read_success and cleanup_success:\n",
    "            return log_test_result(\"File System Operations\", \"PASS\",\n",
    "                                 \"All file operations successful\", details)\n",
    "        else:\n",
    "            return log_test_result(\"File System Operations\", \"FAIL\",\n",
    "                                 \"Some file operations failed\", details)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return log_test_result(\"File System Operations\", \"FAIL\", \n",
    "                             f\"File system test error: {str(e)}\")\n",
    "\n",
    "# Run Jupyter connectivity tests\n",
    "print(\"üì° Running Jupyter Connectivity Tests...\\n\")\n",
    "\n",
    "test_jupyter_server()\n",
    "test_kernel_functionality() \n",
    "test_filesystem_operations()\n",
    "\n",
    "print(f\"\\nüìä Jupyter Tests Completed: {len([r for r in test_results if r['test_name'] in ['Jupyter Server Status', 'Kernel Functionality', 'File System Operations']])} tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5302cf6",
   "metadata": {},
   "source": [
    "## 5. Run End-to-End Integration Tests\n",
    "\n",
    "Execute comprehensive tests including data processing and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 7: Data Processing Pipeline\n",
    "def test_data_processing():\n",
    "    \"\"\"Test complete data science workflow\"\"\"\n",
    "    try:\n",
    "        # Create sample dataset\n",
    "        np.random.seed(42)\n",
    "        data = {\n",
    "            'feature_1': np.random.randn(100),\n",
    "            'feature_2': np.random.randn(100),\n",
    "            'category': np.random.choice(['A', 'B', 'C'], 100),\n",
    "            'target': np.random.randint(0, 2, 100)\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Test pandas operations\n",
    "        df_summary = df.describe()\n",
    "        df_grouped = df.groupby('category').mean()\n",
    "        \n",
    "        # Test numpy operations\n",
    "        correlation_matrix = np.corrcoef(df['feature_1'], df['feature_2'])\n",
    "        \n",
    "        details = {\n",
    "            \"dataframe_shape\": df.shape,\n",
    "            \"summary_stats\": df_summary.shape,\n",
    "            \"grouped_data\": df_grouped.shape,\n",
    "            \"correlation\": float(correlation_matrix[0, 1]),\n",
    "            \"data_types\": list(df.dtypes.astype(str))\n",
    "        }\n",
    "        \n",
    "        # Validate results\n",
    "        tests_passed = [\n",
    "            df.shape == (100, 4),\n",
    "            df_summary.shape[1] == 3,  # 3 numeric columns\n",
    "            len(df_grouped) == 3,      # 3 categories\n",
    "            -1 <= correlation_matrix[0, 1] <= 1  # Valid correlation\n",
    "        ]\n",
    "        \n",
    "        if all(tests_passed):\n",
    "            return log_test_result(\"Data Processing Pipeline\", \"PASS\",\n",
    "                                 \"All data operations successful\", details)\n",
    "        else:\n",
    "            return log_test_result(\"Data Processing Pipeline\", \"FAIL\",\n",
    "                                 f\"Failed validations: {sum(tests_passed)}/{len(tests_passed)}\", details)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return log_test_result(\"Data Processing Pipeline\", \"FAIL\", \n",
    "                             f\"Data processing error: {str(e)}\")\n",
    "\n",
    "# Test 8: Visualization Capabilities\n",
    "def test_visualization():\n",
    "    \"\"\"Test matplotlib and plotly visualization\"\"\"\n",
    "    try:\n",
    "        # Create test data\n",
    "        x = np.linspace(0, 10, 50)\n",
    "        y = np.sin(x)\n",
    "        \n",
    "        # Test matplotlib\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.plot(x, y, label='sin(x)')\n",
    "        ax.set_title('Test Plot')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Save matplotlib figure\n",
    "        output_path = Path(\"/home/jovyan/work/outputs/figures\")\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        mpl_file = output_path / \"test_matplotlib.png\"\n",
    "        plt.savefig(mpl_file, dpi=100, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        mpl_success = mpl_file.exists()\n",
    "        \n",
    "        # Test plotly\n",
    "        fig_plotly = go.Figure()\n",
    "        fig_plotly.add_trace(go.Scatter(x=x, y=y, mode='lines', name='sin(x)'))\n",
    "        fig_plotly.update_layout(title='Test Plotly Plot')\n",
    "        \n",
    "        # Save plotly figure\n",
    "        plotly_file = output_path / \"test_plotly.html\"\n",
    "        fig_plotly.write_html(plotly_file)\n",
    "        \n",
    "        plotly_success = plotly_file.exists()\n",
    "        \n",
    "        details = {\n",
    "            \"matplotlib_file_created\": mpl_success,\n",
    "            \"matplotlib_file_path\": str(mpl_file),\n",
    "            \"plotly_file_created\": plotly_success,\n",
    "            \"plotly_file_path\": str(plotly_file),\n",
    "            \"output_directory\": str(output_path)\n",
    "        }\n",
    "        \n",
    "        if mpl_success and plotly_success:\n",
    "            return log_test_result(\"Visualization Capabilities\", \"PASS\",\n",
    "                                 \"Both matplotlib and plotly working\", details)\n",
    "        else:\n",
    "            return log_test_result(\"Visualization Capabilities\", \"FAIL\",\n",
    "                                 f\"Matplotlib: {mpl_success}, Plotly: {plotly_success}\", details)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return log_test_result(\"Visualization Capabilities\", \"FAIL\", \n",
    "                             f\"Visualization error: {str(e)}\")\n",
    "\n",
    "# Test 9: Machine Learning Workflow\n",
    "def test_machine_learning():\n",
    "    \"\"\"Test basic machine learning workflow\"\"\"\n",
    "    try:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        # Load sample dataset\n",
    "        iris = datasets.load_iris()\n",
    "        X, y = iris.data, iris.target\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Save model results\n",
    "        model_output_dir = Path(\"/home/jovyan/work/outputs/models\")\n",
    "        model_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        import pickle\n",
    "        model_file = model_output_dir / \"test_model.pkl\"\n",
    "        with open(model_file, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "        model_saved = model_file.exists()\n",
    "        \n",
    "        details = {\n",
    "            \"dataset_shape\": X.shape,\n",
    "            \"train_size\": X_train.shape[0],\n",
    "            \"test_size\": X_test.shape[0],\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"model_saved\": model_saved,\n",
    "            \"model_file\": str(model_file)\n",
    "        }\n",
    "        \n",
    "        # Validate results\n",
    "        tests_passed = [\n",
    "            accuracy > 0.8,  # Reasonable accuracy\n",
    "            model_saved,     # Model saved successfully\n",
    "            len(y_pred) == len(y_test)  # Predictions match test set\n",
    "        ]\n",
    "        \n",
    "        if all(tests_passed):\n",
    "            return log_test_result(\"Machine Learning Workflow\", \"PASS\",\n",
    "                                 f\"ML pipeline successful (accuracy: {accuracy:.3f})\", details)\n",
    "        else:\n",
    "            return log_test_result(\"Machine Learning Workflow\", \"FAIL\",\n",
    "                                 f\"Failed ML tests: {sum(tests_passed)}/{len(tests_passed)}\", details)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return log_test_result(\"Machine Learning Workflow\", \"FAIL\", \n",
    "                             f\"ML workflow error: {str(e)}\")\n",
    "\n",
    "# Run end-to-end integration tests\n",
    "print(\"üîÑ Running End-to-End Integration Tests...\\n\")\n",
    "\n",
    "test_data_processing()\n",
    "test_visualization()\n",
    "test_machine_learning()\n",
    "\n",
    "print(f\"\\nüìä Integration Tests Completed: {len([r for r in test_results if r['test_name'] in ['Data Processing Pipeline', 'Visualization Capabilities', 'Machine Learning Workflow']])} tests run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaad758",
   "metadata": {},
   "source": [
    "## 6. Generate Test Reports\n",
    "\n",
    "Create comprehensive test reports with metrics and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd6bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive test report\n",
    "def generate_test_summary():\n",
    "    \"\"\"Generate summary statistics and analysis\"\"\"\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_tests = len(test_results)\n",
    "    passed_tests = len([r for r in test_results if r['status'] == 'PASS'])\n",
    "    failed_tests = len([r for r in test_results if r['status'] == 'FAIL'])\n",
    "    warning_tests = len([r for r in test_results if r['status'] == 'WARN'])\n",
    "    \n",
    "    success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0\n",
    "    \n",
    "    # Categorize tests by type\n",
    "    test_categories = {\n",
    "        'Docker Environment': ['Container Environment', 'Volume Mounts', 'Data Science Packages'],\n",
    "        'Jupyter Functionality': ['Jupyter Server Status', 'Kernel Functionality', 'File System Operations'],\n",
    "        'Data Science Workflow': ['Data Processing Pipeline', 'Visualization Capabilities', 'Machine Learning Workflow']\n",
    "    }\n",
    "    \n",
    "    category_results = {}\n",
    "    for category, test_names in test_categories.items():\n",
    "        category_tests = [r for r in test_results if r['test_name'] in test_names]\n",
    "        category_passed = len([r for r in category_tests if r['status'] == 'PASS'])\n",
    "        category_total = len(category_tests)\n",
    "        category_rate = (category_passed / category_total) * 100 if category_total > 0 else 0\n",
    "        \n",
    "        category_results[category] = {\n",
    "            'passed': category_passed,\n",
    "            'total': category_total,\n",
    "            'success_rate': category_rate\n",
    "        }\n",
    "    \n",
    "    # Create summary object\n",
    "    summary = {\n",
    "        'test_run_info': {\n",
    "            'timestamp': TEST_CONFIG['test_timestamp'],\n",
    "            'total_tests': total_tests,\n",
    "            'passed': passed_tests,\n",
    "            'failed': failed_tests,\n",
    "            'warnings': warning_tests,\n",
    "            'success_rate': success_rate\n",
    "        },\n",
    "        'category_breakdown': category_results,\n",
    "        'detailed_results': test_results\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Display test summary\n",
    "summary = generate_test_summary()\n",
    "\n",
    "print(\"üìä TEST SUMMARY REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Tests Run: {summary['test_run_info']['total_tests']}\")\n",
    "print(f\"‚úÖ Passed: {summary['test_run_info']['passed']}\")\n",
    "print(f\"‚ùå Failed: {summary['test_run_info']['failed']}\")\n",
    "print(f\"‚ö†Ô∏è  Warnings: {summary['test_run_info']['warnings']}\")\n",
    "print(f\"üéØ Success Rate: {summary['test_run_info']['success_rate']:.1f}%\")\n",
    "\n",
    "print(f\"\\nüìà CATEGORY BREAKDOWN\")\n",
    "print(\"-\" * 30)\n",
    "for category, results in summary['category_breakdown'].items():\n",
    "    status_icon = \"‚úÖ\" if results['success_rate'] == 100 else \"‚ö†Ô∏è\" if results['success_rate'] >= 80 else \"‚ùå\"\n",
    "    print(f\"{status_icon} {category}: {results['passed']}/{results['total']} ({results['success_rate']:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìã DETAILED RESULTS\")\n",
    "print(\"-\" * 30)\n",
    "for result in test_results:\n",
    "    status_icon = \"‚úÖ\" if result['status'] == 'PASS' else \"‚ùå\" if result['status'] == 'FAIL' else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status_icon} Test {result['test_id']}: {result['test_name']}\")\n",
    "    if result['message']:\n",
    "        print(f\"   ‚îî‚îÄ‚îÄ {result['message']}\")\n",
    "\n",
    "# Visual summary with matplotlib\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Test results pie chart\n",
    "labels = ['Passed', 'Failed', 'Warnings']\n",
    "sizes = [summary['test_run_info']['passed'], \n",
    "         summary['test_run_info']['failed'], \n",
    "         summary['test_run_info']['warnings']]\n",
    "colors = ['#2ecc71', '#e74c3c', '#f39c12']\n",
    "\n",
    "# Only show non-zero categories\n",
    "non_zero_data = [(label, size, color) for label, size, color in zip(labels, sizes, colors) if size > 0]\n",
    "if non_zero_data:\n",
    "    labels, sizes, colors = zip(*non_zero_data)\n",
    "    ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Overall Test Results')\n",
    "\n",
    "# Category success rates bar chart\n",
    "categories = list(summary['category_breakdown'].keys())\n",
    "success_rates = [summary['category_breakdown'][cat]['success_rate'] for cat in categories]\n",
    "\n",
    "bars = ax2.bar(range(len(categories)), success_rates, color=['#2ecc71' if rate == 100 else '#f39c12' if rate >= 80 else '#e74c3c' for rate in success_rates])\n",
    "ax2.set_xlabel('Test Categories')\n",
    "ax2.set_ylabel('Success Rate (%)')\n",
    "ax2.set_title('Success Rate by Category')\n",
    "ax2.set_xticks(range(len(categories)))\n",
    "ax2.set_xticklabels(categories, rotation=45, ha='right')\n",
    "ax2.set_ylim(0, 105)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, rate) in enumerate(zip(bars, success_rates)):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{rate:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the summary plot\n",
    "plot_path = Path(\"/home/jovyan/work/outputs/figures/test_summary.png\")\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüíæ Summary plot saved to: {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736aafc0",
   "metadata": {},
   "source": [
    "## 7. Save Test Results\n",
    "\n",
    "Save all test results and scripts to the appropriate directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382068de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed test results to JSON\n",
    "timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = Path(TEST_CONFIG[\"results_dir\"]) / f\"integration_test_results_{timestamp_str}.json\"\n",
    "\n",
    "# Save JSON results\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üíæ Test results saved to: {results_file}\")\n",
    "\n",
    "# Create a human-readable test report\n",
    "report_file = Path(TEST_CONFIG[\"results_dir\"]) / f\"test_report_{timestamp_str}.txt\"\n",
    "\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(\"DOCKER COMPOSE JUPYTER LAB INTEGRATION TEST REPORT\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Test Run Timestamp: {summary['test_run_info']['timestamp']}\\n\")\n",
    "    f.write(f\"Total Tests: {summary['test_run_info']['total_tests']}\\n\")\n",
    "    f.write(f\"Passed: {summary['test_run_info']['passed']}\\n\")\n",
    "    f.write(f\"Failed: {summary['test_run_info']['failed']}\\n\") \n",
    "    f.write(f\"Warnings: {summary['test_run_info']['warnings']}\\n\")\n",
    "    f.write(f\"Success Rate: {summary['test_run_info']['success_rate']:.1f}%\\n\\n\")\n",
    "    \n",
    "    f.write(\"CATEGORY BREAKDOWN\\n\")\n",
    "    f.write(\"-\" * 30 + \"\\n\")\n",
    "    for category, results in summary['category_breakdown'].items():\n",
    "        f.write(f\"{category}: {results['passed']}/{results['total']} ({results['success_rate']:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(f\"\\nDETAILED TEST RESULTS\\n\")\n",
    "    f.write(\"-\" * 30 + \"\\n\")\n",
    "    for result in test_results:\n",
    "        f.write(f\"Test {result['test_id']}: {result['test_name']} - {result['status']}\\n\")\n",
    "        if result['message']:\n",
    "            f.write(f\"  Message: {result['message']}\\n\")\n",
    "        if result['details']:\n",
    "            f.write(f\"  Details: {json.dumps(result['details'], indent=4)}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"üìÑ Human-readable report saved to: {report_file}\")\n",
    "\n",
    "# Create a test script template for future use\n",
    "test_script_template = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Docker Compose Jupyter Lab Test Script Template\n",
    "Generated automatically from integration test notebook\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def run_docker_compose_tests():\n",
    "    \"\"\"Run basic Docker Compose validation tests\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Test 1: Check Docker Compose configuration\n",
    "    try:\n",
    "        result = subprocess.run(['docker', 'compose', 'config'], \n",
    "                              capture_output=True, text=True, timeout=30)\n",
    "        if result.returncode == 0:\n",
    "            results.append({\"test\": \"compose_config\", \"status\": \"PASS\"})\n",
    "        else:\n",
    "            results.append({\"test\": \"compose_config\", \"status\": \"FAIL\", \"error\": result.stderr})\n",
    "    except Exception as e:\n",
    "        results.append({\"test\": \"compose_config\", \"status\": \"FAIL\", \"error\": str(e)})\n",
    "    \n",
    "    # Test 2: Check if services are running\n",
    "    try:\n",
    "        result = subprocess.run(['docker', 'compose', 'ps'], \n",
    "                              capture_output=True, text=True, timeout=30)\n",
    "        if result.returncode == 0 and \"jupyterlab-datascience\" in result.stdout:\n",
    "            results.append({\"test\": \"service_status\", \"status\": \"PASS\"})\n",
    "        else:\n",
    "            results.append({\"test\": \"service_status\", \"status\": \"FAIL\", \"output\": result.stdout})\n",
    "    except Exception as e:\n",
    "        results.append({\"test\": \"service_status\", \"status\": \"FAIL\", \"error\": str(e)})\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running Docker Compose tests...\")\n",
    "    results = run_docker_compose_tests()\n",
    "    \n",
    "    for result in results:\n",
    "        status_icon = \"‚úÖ\" if result[\"status\"] == \"PASS\" else \"‚ùå\"\n",
    "        print(f\"{status_icon} {result['test']}: {result['status']}\")\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = Path(\"results\") / f\"quick_test_{timestamp}.json\"\n",
    "    results_file.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to: {results_file}\")\n",
    "'''\n",
    "\n",
    "script_file = Path(TEST_CONFIG[\"project_root\"]) / \"scripts\" / \"tests\" / \"docker_compose_validator.py\"\n",
    "with open(script_file, 'w') as f:\n",
    "    f.write(test_script_template)\n",
    "\n",
    "print(f\"üîß Test script template saved to: {script_file}\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\nüéâ INTEGRATION TEST COMPLETE!\")\n",
    "print(f\"üìÅ All files saved to: {Path(TEST_CONFIG['results_dir'])}\")\n",
    "print(f\"üìä Overall Success Rate: {summary['test_run_info']['success_rate']:.1f}%\")\n",
    "\n",
    "if summary['test_run_info']['success_rate'] == 100:\n",
    "    print(\"‚úÖ All tests passed! Your Docker Compose Jupyter Lab setup is working perfectly!\")\n",
    "elif summary['test_run_info']['success_rate'] >= 80:\n",
    "    print(\"‚ö†Ô∏è Most tests passed! Minor issues detected - check the detailed results.\")\n",
    "else:\n",
    "    print(\"‚ùå Several tests failed! Review the detailed results and fix the issues.\")\n",
    "\n",
    "# List all generated files\n",
    "print(f\"\\nüìÑ Generated Files:\")\n",
    "generated_files = [\n",
    "    results_file,\n",
    "    report_file, \n",
    "    script_file,\n",
    "    \"/home/jovyan/work/outputs/figures/test_summary.png\"\n",
    "]\n",
    "\n",
    "for file_path in generated_files:\n",
    "    if Path(file_path).exists():\n",
    "        print(f\"   ‚úÖ {file_path}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {file_path} (not found)\")\n",
    "\n",
    "print(f\"\\nüèÅ Integration testing completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
